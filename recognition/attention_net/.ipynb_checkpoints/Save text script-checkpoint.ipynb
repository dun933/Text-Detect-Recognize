{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(alphabet=' 0123456789abcdefghijklmnopqrstuvwxyz', batch_size=1, context_vector=False, gpu='1', lexicon=None, load_epoch=0, load_folder='/users/czhang/data/FAN/', load_height=32, load_iter=0, load_width=256, max_ed=3, max_len=65, min_gt_len=3, name='second_training_bn', num_workers=32, out_dir='/scratch/shared/slow/yangl/code/attention_net/output/', root='/users/czhang/data/', tbx_folder='/scratch/shared/slow/yangl/code/attention_net/tbx/', test_dataset='ic13')\n",
      "2019-04-18 16:25:58,722 [INFO ]  Logging file is /scratch/shared/slow/yangl/code/attention_net/output/ic13.txt\n",
      "2019-04-18 16:25:58,724 [INFO ]  model will be evaluated on ic13\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "%run save_train_txt.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing save_train_txt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile save_train_txt.py\n",
    "import os\n",
    "import os.path as osp \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "from utils.utils import setup_logger, print_args, strLabelConverter, lex_free_acc, lex_acc\n",
    "from model import AN\n",
    "from dataloader.SynthLoader import text_collate\n",
    "from dataloader.SceneLoader import SceneLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "parser = argparse.ArgumentParser(description='AN')\n",
    "parser.add_argument('--name', default='second_training_bn', type=str)\n",
    "\n",
    "## data setting \n",
    "parser.add_argument('--root', default='/users/czhang/data/',type=str)\n",
    "parser.add_argument('--load_folder', default='/users/czhang/data/FAN/', type=str)\n",
    "parser.add_argument('--test_dataset', default='ic13', type=str)\n",
    "parser.add_argument('--load_width', default=256, type=int)\n",
    "parser.add_argument('--load_height', default=32, type=int)\n",
    "parser.add_argument('--batch_size', default=1, type=int)\n",
    "parser.add_argument('--num_workers', default=32, type=int)\n",
    "parser.add_argument(\"--gpus\", dest=\"gpu\", default=\"1\", type=str)\n",
    "parser.add_argument('--min_gt_len', default =3, type = int)\n",
    "parser.add_argument('--max_len', default=65, type=int)\n",
    "parser.add_argument(\"--cv\", dest=\"context_vector\", action = 'store_true')\n",
    "parser.add_argument('--lexicon',default = None, type = str)\n",
    "parser.add_argument('--max_ed',default = 3, type = int)\n",
    "parser.add_argument('--tbx_folder', default='/scratch/shared/slow/yangl/code/attention_net/tbx/', type=str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## model setting\n",
    "parser.add_argument('--load_epoch', default=0, type=int)\n",
    "parser.add_argument('--load_iter', default=0, type=int)\n",
    "\n",
    "parser.add_argument('--alphabet', default=' 0123456789abcdefghijklmnopqrstuvwxyz', type=str)\n",
    "## output setting\n",
    "parser.add_argument('--out_dir', default='/scratch/shared/slow/yangl/code/attention_net/output/', type=str)\n",
    "args, unknown = parser.parse_known_args()\n",
    "print (args)\n",
    "\n",
    "\n",
    "\n",
    "args.nClasses = len(args.alphabet)\n",
    "args.load_folder = osp.join(args.load_folder ,args.name)\n",
    "#args.out_dir = osp.join(args.out_dir ,args.name,'tests')\n",
    "#args.out_dir = osp.join(args.out_dir ,args.name,'tests')\n",
    "if not osp.exists(args.out_dir):\n",
    "    os.mkdir(args.out_dir)\n",
    "\n",
    "#tbx_dir =osp.join(args.tbx_folder,args.name,'tests')\n",
    "tbx_dir =args.tbx_folder\n",
    "if osp.exists(args.tbx_folder) == False:\n",
    "    os.mkdir(args.tbx_folder)\n",
    "\n",
    "if osp.exists(tbx_dir) == False:\n",
    "    os.mkdir(tbx_dir)\n",
    "\n",
    "writer = SummaryWriter(tbx_dir)\n",
    "\n",
    "log_path = os.path.join(args.out_dir, args.test_dataset + '.txt')\n",
    "\n",
    "setup_logger(log_path)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "logging.info('model will be evaluated on %s'%(args.test_dataset))\n",
    "\n",
    "\n",
    "net = AN(args)\n",
    "net = torch.nn.DataParallel(net).to(device)\n",
    "checkpoint = '../attention_net/0_480000.pth'\n",
    "\n",
    "load_file= torch.load(checkpoint)\n",
    "net.load_state_dict(load_file['model_state_dict'])\n",
    "\n",
    "#net.load_state_dict(torch.load(load_file))\n",
    "net.eval()\n",
    "n_correct = 0\n",
    "skip_counter = 0\n",
    "converter = strLabelConverter(args.alphabet)\n",
    "\n",
    "\n",
    "\n",
    "pickle_path ='/scratch/shared/slow/yangl/code/textinvideo/pixel_link/file_train_box.pkl'\n",
    "with open (pickle_path,'rb') as p:\n",
    "    data=pickle.load(p)\n",
    "    \n",
    "text_result={}\n",
    "all_keys= list(data.keys())\n",
    "for kk in range (len(all_keys)):\n",
    "    \n",
    "    temp_key =all_keys[kk].split('/')[-2] \n",
    "    text_result[temp_key]=[]\n",
    "\n",
    "#all_keys= list(data.keys())[261319:261379]  #1002\n",
    "#all_keys= list(data.keys())[1627:1686]   #5499\n",
    "#all_keys= list(data.keys())[13429:13514]   #2517\n",
    "width, height = args.load_width, args.load_height\n",
    "progress = 0\n",
    "for img_id in range (len(all_keys))    :\n",
    "    img_path = all_keys[img_id]\n",
    "    all_boxes = data[img_path]\n",
    "    #temp_box = all_boxes[9]\n",
    "\n",
    "    img_raw = cv2.imread(img_path)\n",
    "    temp_key =img_path.split('/')[-2] \n",
    "    #print (temp_key)\n",
    "    for kk in range (len(all_boxes)):\n",
    "\n",
    "        temp_box = all_boxes[kk]\n",
    "        odd= np.asarray([temp_box[0], temp_box[2],temp_box[4], temp_box[6]])\n",
    "        even= np.asarray([temp_box[1], temp_box[3],temp_box[5], temp_box[7]])\n",
    "\n",
    "        y_min = min(odd)\n",
    "        y_max = max(odd)\n",
    "\n",
    "        x_min = min(even)\n",
    "        x_max = max(even)\n",
    "\n",
    "\n",
    "        new_img = img_raw.copy()\n",
    "        new_img=new_img[x_min:x_max,y_min:y_max, :]\n",
    "\n",
    "\n",
    "        img = cv2.resize(new_img.copy(), (width, height))\n",
    "        img = img[:, :, (2, 1, 0)] ## rgb\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        imgs = torch.from_numpy(img).permute(2, 0, 1).float()\n",
    "        imgs= imgs.view(1, 3,32,256)\n",
    "        imgs = Variable(imgs).cuda()\n",
    "        gt_ind,_ = converter.encode('abc')\n",
    "        gt_ind = torch.IntTensor((gt_ind + [0] * args.max_len)[:args.max_len])\n",
    "        preds = net(imgs,gt_ind)\n",
    "\n",
    "        preds_label = torch.argmax(preds,dim =2)\n",
    "        preds_conf = torch.max(preds,dim =2)\n",
    "\n",
    "        word_len = torch.sum(preds_label!=0)\n",
    "        conf_score = torch.mean(preds_conf[0][0][0:word_len])\n",
    "\n",
    "        _,pred_str,_= lex_free_acc(preds,gt_ind,converter)\n",
    "        #print (pred_str)\n",
    "        #print (conf_score.cpu().detach().numpy())\n",
    "        if conf_score>0.95:\n",
    "            text_result[temp_key].append(pred_str)\n",
    "            #print (pred_str)\n",
    "            \n",
    "        if progress%1000 ==0 :\n",
    "            print (progress)\n",
    "        progress=progress+1\n",
    "        \n",
    "        \n",
    "        \n",
    "final_keys = list(text_result.keys())\n",
    "for temp_key in final_keys:\n",
    "    text_result[temp_key]= list(set(text_result[temp_key]))\n",
    "    if len(list(set(text_result[temp_key])))>0:\n",
    "        print (temp_key)\n",
    "\n",
    "        \n",
    "save_pickle_path ='/scratch/shared/slow/yangl/code/textinvideo/MSR_VTT_train_text.pkl'\n",
    "with open (save_pickle_path,'wb') as f:\n",
    "    pickle.dump(text_result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile save_test_txt.py\n",
    "import os\n",
    "import os.path as osp \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "from utils.utils import setup_logger, print_args, strLabelConverter, lex_free_acc, lex_acc\n",
    "from model import AN\n",
    "from dataloader.SynthLoader import text_collate\n",
    "from dataloader.SceneLoader import SceneLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "parser = argparse.ArgumentParser(description='AN')\n",
    "parser.add_argument('--name', default='second_training_bn', type=str)\n",
    "\n",
    "## data setting \n",
    "parser.add_argument('--root', default='/users/czhang/data/',type=str)\n",
    "parser.add_argument('--load_folder', default='/users/czhang/data/FAN/', type=str)\n",
    "parser.add_argument('--test_dataset', default='ic13', type=str)\n",
    "parser.add_argument('--load_width', default=256, type=int)\n",
    "parser.add_argument('--load_height', default=32, type=int)\n",
    "parser.add_argument('--batch_size', default=1, type=int)\n",
    "parser.add_argument('--num_workers', default=32, type=int)\n",
    "parser.add_argument(\"--gpus\", dest=\"gpu\", default=\"2\", type=str)\n",
    "parser.add_argument('--min_gt_len', default =3, type = int)\n",
    "parser.add_argument('--max_len', default=65, type=int)\n",
    "parser.add_argument(\"--cv\", dest=\"context_vector\", action = 'store_true')\n",
    "parser.add_argument('--lexicon',default = None, type = str)\n",
    "parser.add_argument('--max_ed',default = 3, type = int)\n",
    "parser.add_argument('--tbx_folder', default='/scratch/shared/slow/yangl/code/attention_net/tbx/', type=str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## model setting\n",
    "parser.add_argument('--load_epoch', default=0, type=int)\n",
    "parser.add_argument('--load_iter', default=0, type=int)\n",
    "\n",
    "parser.add_argument('--alphabet', default=' 0123456789abcdefghijklmnopqrstuvwxyz', type=str)\n",
    "## output setting\n",
    "parser.add_argument('--out_dir', default='/scratch/shared/slow/yangl/code/attention_net/output/', type=str)\n",
    "args, unknown = parser.parse_known_args()\n",
    "print (args)\n",
    "\n",
    "\n",
    "\n",
    "args.nClasses = len(args.alphabet)\n",
    "args.load_folder = osp.join(args.load_folder ,args.name)\n",
    "#args.out_dir = osp.join(args.out_dir ,args.name,'tests')\n",
    "#args.out_dir = osp.join(args.out_dir ,args.name,'tests')\n",
    "if not osp.exists(args.out_dir):\n",
    "    os.mkdir(args.out_dir)\n",
    "\n",
    "#tbx_dir =osp.join(args.tbx_folder,args.name,'tests')\n",
    "tbx_dir =args.tbx_folder\n",
    "if osp.exists(args.tbx_folder) == False:\n",
    "    os.mkdir(args.tbx_folder)\n",
    "\n",
    "if osp.exists(tbx_dir) == False:\n",
    "    os.mkdir(tbx_dir)\n",
    "\n",
    "writer = SummaryWriter(tbx_dir)\n",
    "\n",
    "log_path = os.path.join(args.out_dir, args.test_dataset + '.txt')\n",
    "\n",
    "setup_logger(log_path)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.gpu\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "logging.info('model will be evaluated on %s'%(args.test_dataset))\n",
    "\n",
    "\n",
    "net = AN(args)\n",
    "net = torch.nn.DataParallel(net).to(device)\n",
    "checkpoint = '../attention_net/0_480000.pth'\n",
    "\n",
    "load_file= torch.load(checkpoint)\n",
    "net.load_state_dict(load_file['model_state_dict'])\n",
    "\n",
    "#net.load_state_dict(torch.load(load_file))\n",
    "net.eval()\n",
    "n_correct = 0\n",
    "skip_counter = 0\n",
    "converter = strLabelConverter(args.alphabet)\n",
    "\n",
    "\n",
    "\n",
    "pickle_path ='/scratch/shared/slow/yangl/code/textinvideo/pixel_link/file_test_box.pkl'\n",
    "with open (pickle_path,'rb') as p:\n",
    "    data=pickle.load(p)\n",
    "    \n",
    "text_result={}\n",
    "all_keys= list(data.keys())\n",
    "for kk in range (len(all_keys)):\n",
    "    \n",
    "    temp_key =all_keys[kk].split('/')[-2] \n",
    "    text_result[temp_key]=[]\n",
    "\n",
    "#all_keys= list(data.keys())[261319:261379]  #1002\n",
    "#all_keys= list(data.keys())[1627:1686]   #5499\n",
    "#all_keys= list(data.keys())[13429:13514]   #2517\n",
    "width, height = args.load_width, args.load_height\n",
    "progress = 0\n",
    "for img_id in range (len(all_keys))    :\n",
    "    img_path = all_keys[img_id]\n",
    "    all_boxes = data[img_path]\n",
    "    #temp_box = all_boxes[9]\n",
    "\n",
    "    img_raw = cv2.imread(img_path)\n",
    "    temp_key =img_path.split('/')[-2] \n",
    "    #print (temp_key)\n",
    "    for kk in range (len(all_boxes)):\n",
    "\n",
    "        temp_box = all_boxes[kk]\n",
    "        odd= np.asarray([temp_box[0], temp_box[2],temp_box[4], temp_box[6]])\n",
    "        even= np.asarray([temp_box[1], temp_box[3],temp_box[5], temp_box[7]])\n",
    "\n",
    "        y_min = min(odd)\n",
    "        y_max = max(odd)\n",
    "\n",
    "        x_min = min(even)\n",
    "        x_max = max(even)\n",
    "\n",
    "\n",
    "        new_img = img_raw.copy()\n",
    "        new_img=new_img[x_min:x_max,y_min:y_max, :]\n",
    "\n",
    "\n",
    "        img = cv2.resize(new_img.copy(), (width, height))\n",
    "        img = img[:, :, (2, 1, 0)] ## rgb\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "        imgs = torch.from_numpy(img).permute(2, 0, 1).float()\n",
    "        imgs= imgs.view(1, 3,32,256)\n",
    "        imgs = Variable(imgs).cuda()\n",
    "        gt_ind,_ = converter.encode('abc')\n",
    "        gt_ind = torch.IntTensor((gt_ind + [0] * args.max_len)[:args.max_len])\n",
    "        preds = net(imgs,gt_ind)\n",
    "\n",
    "        preds_label = torch.argmax(preds,dim =2)\n",
    "        preds_conf = torch.max(preds,dim =2)\n",
    "\n",
    "        word_len = torch.sum(preds_label!=0)\n",
    "        conf_score = torch.mean(preds_conf[0][0][0:word_len])\n",
    "\n",
    "        _,pred_str,_= lex_free_acc(preds,gt_ind,converter)\n",
    "        #print (pred_str)\n",
    "        #print (conf_score.cpu().detach().numpy())\n",
    "        if conf_score>0.95:\n",
    "            text_result[temp_key].append(pred_str)\n",
    "            #print (pred_str)\n",
    "            \n",
    "        if progress%1000 ==0 :\n",
    "            print (progress)\n",
    "        progress=progress+1\n",
    "        \n",
    "        \n",
    "        \n",
    "final_keys = list(text_result.keys())\n",
    "for temp_key in final_keys:\n",
    "    text_result[temp_key]= list(set(text_result[temp_key]))\n",
    "    if len(list(set(text_result[temp_key])))>0:\n",
    "        print (temp_key)\n",
    "\n",
    "        \n",
    "save_pickle_path ='/scratch/shared/slow/yangl/code/textinvideo/MSR_VTT_test_text.pkl'\n",
    "with open (save_pickle_path,'wb') as f:\n",
    "    pickle.dump(text_result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
